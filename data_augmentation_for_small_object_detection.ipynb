{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stitcher Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching labels (13467 found, 0 missing, 0 empty, 0 duplicate, for 13467 images): 100%|██████████| 13467/13467 [00:01<00:00, 10214.01it/s]\n"
     ]
    }
   ],
   "source": [
    "hyp = {'giou': 3.54,  # giou loss gain\n",
    "       'cls': 37.4,  # cls loss gain\n",
    "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
    "       'obj': 64.3,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
    "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
    "       'iou_t': 0.20,  # iou training threshold\n",
    "       'lr0': 0.01,  # initial learning rate (SGD=5E-3, Adam=5E-4)\n",
    "       'lrf': 0.0005,  # final learning rate (with cos scheduler)\n",
    "       'momentum': 0.937,  # SGD momentum\n",
    "       'weight_decay': 0.0005,  # optimizer weight decay\n",
    "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
    "       'hsv_h': 0.0138,  # image HSV-Hue augmentation (fraction)\n",
    "       'hsv_s': 0.678,  # image HSV-Saturation augmentation (fraction)\n",
    "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
    "       'degrees': 1.98 * 0,  # image rotation (+/- deg)\n",
    "       'translate': 0.05 * 0,  # image translation (+/- fraction)\n",
    "       'scale': 0.05 * 0,  # image scale (+/- gain)\n",
    "       'shear': 0.641 * 0}  # image shear (+/- deg)\n",
    "\n",
    "\n",
    "train_path = '/store/dataset/rubbish_yolo/train.txt'\n",
    "img_size = 512\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "from my_utils.mydatasets import *\n",
    "\n",
    "\n",
    "# Dataset\n",
    "dataset = LoadImagesAndLabels(train_path, img_size, batch_size,\n",
    "                              augment=True,\n",
    "                              hyp=hyp,  # augmentation hyperparameters\n",
    "                              rect=False,  # rectangular training\n",
    "                              single_cls=False)\n",
    "\n",
    "# # Dataloader\n",
    "batch_size = min(batch_size, len(dataset))\n",
    "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         num_workers=nw,\n",
    "                                         shuffle=True,  # Shuffle=True unless rectangular training is used\n",
    "                                         pin_memory=False,\n",
    "                                         collate_fn=dataset.collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 测试 stitcher 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          ...,\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.]],\n",
      "\n",
      "         [[114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          ...,\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.]],\n",
      "\n",
      "         [[114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          ...,\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.]]],\n",
      "\n",
      "\n",
      "        [[[114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          ...,\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.]],\n",
      "\n",
      "         [[114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          ...,\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.]],\n",
      "\n",
      "         [[114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          ...,\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.]]],\n",
      "\n",
      "\n",
      "        [[[118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          ...,\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.]],\n",
      "\n",
      "         [[118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          ...,\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.]],\n",
      "\n",
      "         [[118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          ...,\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.],\n",
      "          [118., 118., 118.,  ..., 118., 118., 118.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          ...,\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.]],\n",
      "\n",
      "         [[114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          ...,\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.]],\n",
      "\n",
      "         [[114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          ...,\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.],\n",
      "          [114., 114., 114.,  ..., 114., 114., 114.]]],\n",
      "\n",
      "\n",
      "        [[[ 81.,  81.,  81.,  ...,  81.,  81.,  81.],\n",
      "          [ 81.,  81.,  81.,  ...,  81.,  81.,  81.],\n",
      "          [ 81.,  81.,  81.,  ...,  81.,  81.,  81.],\n",
      "          ...,\n",
      "          [108., 107., 106.,  ...,  30.,  28.,  27.],\n",
      "          [105., 106., 107.,  ...,  31.,  31.,  31.],\n",
      "          [102., 103., 105.,  ...,  30.,  30.,  31.]],\n",
      "\n",
      "         [[ 81.,  81.,  81.,  ...,  81.,  81.,  81.],\n",
      "          [ 81.,  81.,  81.,  ...,  81.,  81.,  81.],\n",
      "          [ 81.,  81.,  81.,  ...,  81.,  81.,  81.],\n",
      "          ...,\n",
      "          [ 60.,  59.,  58.,  ...,  19.,  18.,  18.],\n",
      "          [ 61.,  62.,  64.,  ...,  19.,  21.,  22.],\n",
      "          [ 60.,  60.,  61.,  ...,  18.,  20.,  22.]],\n",
      "\n",
      "         [[ 81.,  81.,  81.,  ...,  81.,  81.,  81.],\n",
      "          [ 81.,  81.,  81.,  ...,  81.,  81.,  81.],\n",
      "          [ 81.,  81.,  81.,  ...,  81.,  81.,  81.],\n",
      "          ...,\n",
      "          [ 51.,  51.,  52.,  ...,  23.,  24.,  24.],\n",
      "          [ 53.,  55.,  57.,  ...,  19.,  22.,  24.],\n",
      "          [ 53.,  54.,  56.,  ...,  19.,  21.,  23.]]],\n",
      "\n",
      "\n",
      "        [[[210., 210., 210.,  ..., 138., 138., 139.],\n",
      "          [205., 206., 207.,  ..., 137., 139., 140.],\n",
      "          [197., 198., 200.,  ..., 140., 142., 141.],\n",
      "          ...,\n",
      "          [ 81.,  81.,  80.,  ..., 115., 115., 115.],\n",
      "          [ 83.,  81.,  80.,  ..., 115., 115., 115.],\n",
      "          [ 85.,  85.,  85.,  ..., 115., 115., 115.]],\n",
      "\n",
      "         [[168., 169., 169.,  ..., 156., 156., 157.],\n",
      "          [162., 164., 165.,  ..., 157., 157., 158.],\n",
      "          [154., 155., 158.,  ..., 160., 160., 159.],\n",
      "          ...,\n",
      "          [127., 127., 124.,  ..., 115., 115., 115.],\n",
      "          [127., 126., 125.,  ..., 115., 115., 115.],\n",
      "          [129., 128., 128.,  ..., 115., 115., 115.]],\n",
      "\n",
      "         [[ 84.,  87.,  89.,  ..., 118., 118., 118.],\n",
      "          [ 77.,  80.,  82.,  ..., 118., 118., 119.],\n",
      "          [ 69.,  70.,  75.,  ..., 121., 121., 120.],\n",
      "          ...,\n",
      "          [150., 147., 147.,  ..., 115., 115., 115.],\n",
      "          [149., 146., 148.,  ..., 115., 115., 115.],\n",
      "          [148., 147., 150.,  ..., 115., 115., 115.]]]])\n",
      "torch.Size([16, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "from my_utils.utils import *\n",
    "import torchvision\n",
    "for i, (imgs, targets, paths, _) in enumerate(dataloader):\n",
    "    imgs = imgs.float()\n",
    "    print(imgs)\n",
    "    print(imgs.shape)\n",
    "    plot_images(imgs, targets)\n",
    "    #torchvision.utils.save_image(imgs, 'trainimg.png', nrow=4)\n",
    "    \n",
    "    break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 显示图像和gt框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from my_utils.utils import *\n",
    "def tensor2PIL(t, m = 255):\n",
    "    if m == 1:\n",
    "        t = t * 255\n",
    "    t = t.permute(1,2,0).numpy()\n",
    "    img = Image.fromarray(t.astype(np.uint8))\n",
    "    return img\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Param:\n",
    "    image: PIL.Image\n",
    "    box: array\n",
    "\"\"\"\n",
    "def show_box_on_image(image, boxes):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(list(box), outline='red', width=3)\n",
    "    del draw\n",
    "    \n",
    "\n",
    "index = 3\n",
    "\n",
    "d = dataset[index]\n",
    "\n",
    "img = tensor2PIL(d[0])\n",
    "## xywh\n",
    "boxes = d[1][:, 2:]\n",
    "whwh = torch.tensor([img.size[0], img.size[1], img.size[0], img.size[1]])\n",
    "boxes = xywh2xyxy(boxes) * whwh\n",
    "show_box_on_image(img, boxes)\n",
    "\n",
    "img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 测试 stitcher 数据增广 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(self, index, img_size=None):\n",
    "    if img_size is None:\n",
    "        img_size = self.img_size\n",
    "    # loads 1 image from dataset, returns img, original hw, resized hw\n",
    "    img = self.imgs[index]\n",
    "    if img is None:  # not cached\n",
    "        path = self.img_files[index]\n",
    "        img = cv2.imread(path)  # BGR\n",
    "        assert img is not None, 'Image Not Found ' + path\n",
    "        h0, w0 = img.shape[:2]  # orig hw\n",
    "        r = img_size / max(h0, w0)  # resize image to img_size\n",
    "        if r < 1 or (self.augment and r != 1):  # always resize down, only resize up if training with augmentation\n",
    "        #if r !=1:\n",
    "            interp = cv2.INTER_AREA if r < 1 and not self.augment else cv2.INTER_LINEAR\n",
    "            img = cv2.resize(img, (int(w0 * r), int(h0 * r)), interpolation=interp)\n",
    "        return img, (h0, w0), img.shape[:2]  # img, hw_original, hw_resized\n",
    "    else:\n",
    "        return self.imgs[index], self.img_hw0[index], self.img_hw[index]  # img, hw_original, hw_resized\n",
    "\n",
    "\n",
    "\n",
    "## when training\n",
    "def load_stitcher(self, index):\n",
    "    labels4 = []\n",
    "    shape = self.img_size\n",
    "    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]\n",
    "    \n",
    "    img4 = np.full((shape, shape, 3), 114, dtype=np.uint8)  # base image with 4 tiles\n",
    "    \n",
    "    for i, index in enumerate(indices):\n",
    "        \n",
    "        ## h0, w0 => (256x) [RESIZE]\n",
    "        img, (h0, w0), (h, w) = load_image(self, index, shape // 2) ### shape // 2  => 256\n",
    "        # Letterbox\n",
    "        img, ratio, pad = letterbox(img, shape // 2, auto=False, scaleup=self.augment) # ratio = 1,padw + padh\n",
    "        \n",
    "        ## combine images\n",
    "        ## top_left 0,0, 256,256 (xyxy)\n",
    "        ## b_left 0,256, 256,512\n",
    "        ## top_right 256,0, 512,256\n",
    "        ## b_right 256, 256, 512, 512\n",
    "        ### 赋值顺序\n",
    "        ## 1 3\n",
    "        ## 2 4\n",
    "        xind = (i//2) * (shape//2), (i//2+1) * (shape//2) ## x 列 横坐标\n",
    "        yind = (i%2) * (shape//2), (i%2+1) * (shape//2) ## y 行 纵坐标\n",
    "        img4[yind[0]:yind[1], xind[0]:xind[1]] = img\n",
    "        \n",
    "        ## compute padding\n",
    "        padw = pad[0] + xind[0] \n",
    "        padh = pad[1] + yind[0]\n",
    "        \n",
    "        ## labels\n",
    "        x = self.labels[index]\n",
    "        labels = x.copy()\n",
    "        if x.size > 0:    \n",
    "            labels[:, 1] = ratio[0] * w * (x[:, 1] - x[:, 3] / 2) + padw ## x_min\n",
    "            labels[:, 2] = ratio[1] * h * (x[:, 2] - x[:, 4] / 2) + padh ## y_min\n",
    "            labels[:, 3] = ratio[0] * w * (x[:, 1] + x[:, 3] / 2) + padw ## x_max\n",
    "            labels[:, 4] = ratio[1] * h * (x[:, 2] + x[:, 4] / 2) + padh ## y_max\n",
    "        labels4.append(labels)\n",
    "        \n",
    "    if len(labels4):\n",
    "        labels4 = np.concatenate(labels4, 0)\n",
    "    \n",
    "    return img4, labels4\n",
    "\n",
    "img, labels = load_stitcher(dataset, 1)\n",
    "\n",
    "print(img.shape)\n",
    "from PIL import Image\n",
    "img = img[:,:,::-1]\n",
    "img_pil = Image.fromarray(img)\n",
    "print(labels)\n",
    "show_box_on_image(img_pil, labels[:,1:])\n",
    "img_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 测试 mosaic 数据增广"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mosaic(self, index):\n",
    "    # loads images in a mosaic\n",
    "\n",
    "    labels4 = []\n",
    "    s = self.img_size\n",
    "    xc, yc = [int(random.uniform(s * 0.5, s * 1.5)) for _ in range(2)]  # mosaic center x, y\n",
    "    indices = [index] + [random.randint(0, len(self.labels) - 1) for _ in range(3)]  # 3 additional image indices\n",
    "    for i, index in enumerate(indices):\n",
    "        # Load image\n",
    "        img, _, (h, w) = load_image(self, index)\n",
    "\n",
    "        # place img in img4\n",
    "        if i == 0:  # top left\n",
    "            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n",
    "            print('imgs4 shape:', img4.shape)\n",
    "            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
    "            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
    "        elif i == 1:  # top right\n",
    "            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
    "            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
    "        elif i == 2:  # bottom left\n",
    "            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
    "            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n",
    "        elif i == 3:  # bottom right\n",
    "            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
    "            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
    "\n",
    "        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n",
    "        padw = x1a - x1b\n",
    "        padh = y1a - y1b\n",
    "\n",
    "        # Labels\n",
    "        x = self.labels[index]\n",
    "        labels = x.copy()\n",
    "        if x.size > 0:  # Normalized xywh to pixel xyxy format\n",
    "            labels[:, 1] = w * (x[:, 1] - x[:, 3] / 2) + padw\n",
    "            labels[:, 2] = h * (x[:, 2] - x[:, 4] / 2) + padh\n",
    "            labels[:, 3] = w * (x[:, 1] + x[:, 3] / 2) + padw\n",
    "            labels[:, 4] = h * (x[:, 2] + x[:, 4] / 2) + padh\n",
    "        labels4.append(labels)\n",
    "\n",
    "    # Concat/clip labels\n",
    "    if len(labels4):\n",
    "        labels4 = np.concatenate(labels4, 0)\n",
    "        # np.clip(labels4[:, 1:] - s / 2, 0, s, out=labels4[:, 1:])  # use with center crop\n",
    "        np.clip(labels4[:, 1:], 0, 2 * s, out=labels4[:, 1:])  # use with random_affine\n",
    "\n",
    "\n",
    "    # Augment\n",
    "    # img4 = img4[s // 2: int(s * 1.5), s // 2:int(s * 1.5)]  # center crop (WARNING, requires box pruning)\n",
    "    img4, labels4 = random_affine(img4, labels4,\n",
    "                                  degrees=self.hyp['degrees'],\n",
    "                                  translate=self.hyp['translate'],\n",
    "                                  scale=self.hyp['scale'],\n",
    "                                  shear=self.hyp['shear'],\n",
    "                                  border=-s // 2)  # border to remove\n",
    "\n",
    "    return img4, labels4\n",
    "\n",
    "img, labels = load_mosaic(dataset, 1)\n",
    "\n",
    "print(img.shape)\n",
    "from PIL import Image\n",
    "img = img[:,:,::-1]\n",
    "img_pil = Image.fromarray(img)\n",
    "#print(labels)\n",
    "show_box_on_image(img_pil, labels[:,1:])\n",
    "img_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy-Paste Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stitcher 和 mosaic 的数据增广方式，会使得数据中包含足够多的小样本，\n",
    "所以不再需要这类增加小样本个数的数据增广"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils import augmentation\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "ssdaug = augmentation.SSDAugmentation()\n",
    "\n",
    "index = 3\n",
    "\n",
    "d = dataset[index]\n",
    "\n",
    "img = np.array(tensor2PIL(d[0]))\n",
    "## xywh\n",
    "boxes = d[1][:, 2:].numpy()\n",
    "labels = d[1][:,1].numpy()\n",
    "\n",
    "whwh = np.array([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "boxes = xywh2xyxy(boxes) * whwh\n",
    "print(boxes)\n",
    "img_a, boxes_a, labels_a = ssdaug(img, boxes, labels)\n",
    "\n",
    "img_pil = Image.fromarray(img_a.astype(np.uint8))\n",
    "whwh = np.array([img_pil.size[0], img_pil.size[1], img_pil.size[0], img_pil.size[1]])\n",
    "#boxes_a = boxes_a * whwh\n",
    "print()\n",
    "print(boxes_a)\n",
    "print(len(boxes_a))\n",
    "\n",
    "show_box_on_image(img_pil, boxes_a)\n",
    "\n",
    "print(img_pil.size)\n",
    "img_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
